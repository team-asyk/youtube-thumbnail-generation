{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"sample.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.1"}},"cells":[{"cell_type":"markdown","metadata":{"id":"Fvj2rC--SRfj"},"source":["# The MIT License"]},{"cell_type":"markdown","metadata":{"id":"mmr3mcc7SRfp"},"source":["Copyright 2020 Akari Shimono, Yuki Kakui\n","\n","Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n","\n","The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n","\n","THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."]},{"cell_type":"markdown","metadata":{"id":"hkMuqKjGLXMH"},"source":["# Attention\n","\n","Before running this file, please confirm **API keys (Face API, remove.bg, TSUNA)** are prepared."]},{"cell_type":"code","metadata":{"id":"LCuZJ99i38MT"},"source":["# API Keys\n","faceAPI_key = input(\"API Key of Face: \")\n","removebg_key = input(\"API Key of remove.bg: \")\n","tsuna_key = input(\"API Key of TSUNA: \")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TMdCldbS38MP"},"source":["import cv2\n","import time\n","import glob\n","import requests\n","import argparse, os\n","import bs4, shutil, ssl\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import http.client, urllib.request, urllib.parse, json\n","from bs4 import BeautifulSoup\n","from PIL import Image, ImageDraw, ImageFont"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KDsL7qZm-Vwl"},"source":["video_path = input(\"Video Filename (e.g. ****.mp4): \") #Filename\n","summary_text = input(\"Video Title (Japanese only): \")\n","search_word = input(\"Video Key Phrase (This will be searched on Google): \")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"h2m0McX7LLku"},"source":["# Step 1: Frame Sampling\n","Sampled images are saved in the \"processing/step1\" folder."]},{"cell_type":"code","metadata":{"id":"uUAS5L1_38MW"},"source":["def movie_to_image():\n","    output_path = \"processing/step1/\"\n","    os.makedirs(output_path, exist_ok=True)\n","  \n","    #load the video\n","    capture = cv2.VideoCapture(video_path)\n","    \n","    img_count = 0 #number of sampling images\n","    frame_count = 0 #number of  frames\n","    num_cut  = int(capture.get(cv2.CAP_PROP_FRAME_COUNT)/300) #sample approximately 300 images\n","\n","    while(capture.isOpened()):\n","        ret, frame = capture.read()\n","        if ret == False:\n","            break\n","        if frame_count % num_cut == 0:\n","            img_file_name = output_path + str(img_count) + \".jpg\"\n","            cv2.imwrite(img_file_name, frame)\n","            img_count += 1\n","        frame_count += 1\n","    capture.release()\n","    return img_count\n","\n","img_sum = movie_to_image()\n","print(\"Frame-Sampling Finished.\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DFAmgsD4M0uz"},"source":["# Step 2: Emotion Recognition\n","\n","Free version of Face API recognizes 20 images/min.\n","\n","Two images with the highest probability of happiness and surprise are saved as the \"happiness.jpg\" file and the \"surprise.jpg\" file in the \"processing/step2\" folder."]},{"cell_type":"code","metadata":{"id":"3x08nlxM38MY"},"source":["file = \"processing/step1/\" \n","output_path = \"processing/step2/\"\n","os.makedirs(output_path, exist_ok=True)\n","\n","# extract the image with the highest emotion for each emotion\n","max_emotion = {\"anger\":[\"\", 0.0], \"contempt\":[\"\", 0.0], \"disgust\":[\"\", 0.0], \"fear\":[\"\", 0.0], \n","               \"happiness\":[\"\", 0.0],  \"neutral\":[\"\", 0.0], \"sadness\":[\"\", 0.0], \"surprise\": [\"\", 0.0]}\n","emo_1 = [\"anger\",\"contempt\",\"disgust\",\"fear\",\"happiness\",\"neutral\",\"sadness\",\"surprise\"]\n","img_name = \"\"\n","cnt = 0\n","\n","def RaadJson(datas):\n","    emotion = []\n","    for data in datas:\n","        if data == \"error\":\n","            return 0\n","        f = data[\"faceAttributes\"]\n","        d = f[\"emotion\"]\n","        for name in emo_1:\n","            emotion.append(d[name])\n","    return emotion\n","\n","\n","def Recognize(emotion):\n","    data = np.array(emotion)\n","    if data.size == 0:\n","        return \n","    for num in range(8):\n","        if data[num] > max_emotion[emo_1[num]][1]:\n","            max_emotion[emo_1[num]][0] = img_name\n","            max_emotion[emo_1[num]][1] = data[num]\n","    \n","    \n","headers = {\n","    #Request headers\n","    \"Content-Type\" : \"application/octet-stream\",\n","    \"Ocp-Apim-Subscription-Key\" : faceAPI_key,\n","}\n","\n","params = urllib.parse.urlencode({\n","    #Request parameters\n","    \"returnFaceId\" : \"false\",\n","    'returnFaceLandmarks': 'false',\n","    'returnFaceAttributes': 'emotion'\n","})\n","\n","try:\n","    conn = http.client.HTTPSConnection('westcentralus.api.cognitive.microsoft.com')\n","    for i in range(0, img_sum):\n","        img_name = file + str(i) + \".jpg\"\n","        if cnt > 20:\n","            time.sleep(60)\n","            cnt = 0\n","        f = open(img_name, \"rb\")\n","        conn.request(\"POST\", \"/face/v1.0/detect?%s\" % params, f, headers)\n","        response = conn.getresponse()\n","        data = response.read()\n","        data = json.loads(data)\n","        if RaadJson(data) == 0:\n","            cnt += 1\n","            continue\n","        emotion = RaadJson(data)\n","        Recognize(emotion)\n","        cnt += 1\n","    conn.close()\n","    \n","    for key, value in max_emotion.items():\n","        if key == \"happiness\" or key == \"surprise\":\n","            if value[0] == \"\":\n","                continue\n","            pic = cv2.imread(value[0])\n","            cv2.imwrite(output_path + key + \".jpg\", pic)\n","except Exception as e:\n","    print(\"[Errno {0}] {1}\".format(e.errno, e.strerror))\n","    \n","print(\"Emotion-Recognition Finished.\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"a1tK6s95-Vwp"},"source":["# Step 3: Image Insertion\n","\n","Images dowloaded from Google are saved in the \"processing/step3/search_deta\" folder.\n","\n","Representative object image which background is trimmed is saved as the \"paste_img.png\" file in the \"processing/step3\" folder."]},{"cell_type":"code","metadata":{"id":"ku_1aARl38Mc"},"source":["output_path = \"processing/step3/\"\n","os.makedirs(output_path, exist_ok=True)\n","\n","def detect_face(img):     \n","    face_img = img.copy()  \n","    face_rects = face_cascade.detectMultiScale(face_img) \n","    return face_rects\n","\n","def img_add_msg(img, message, location, emotion, j):\n","    font_path = \"system/NotoSansCJKjp-Bold.otf\" \n","    font_size = 140\n","    font = ImageFont.truetype(font_path, font_size)\n","    img = Image.fromarray(img)\n","    draw = ImageDraw.Draw(img)\n"," \n","    size=draw.textsize(message, font=font)\n","    largex = size[0]\n","    largey = size[1]\n","    #determine where the string must be placed\n","    #priority: bottom → upper left → upper right\n","    length = len(location)\n","    checklu = np.zeros(length)\n","    checkru = np.zeros(length)\n","    checkd = np.zeros(length)\n"," \n","    for i in range(length):\n","        if location[i][1] + location[i][3] + largey + 60 < img.height:\n","            checkd[i] = 1\n","    if all(checkd):\n","        textloc = ((img.width-largex)/2, img.height-largey-60) #bottom\n","    else:\n","        font_size = 60\n","        font = ImageFont.truetype(font_path, font_size)\n","        size2=draw.textsize(message, font=font)\n","        x = size2[0]\n","        y = size2[1]\n","        for i in range(length):\n","            if x + 100 < location[i][0] or y + 60 < location[i][1]:\n","                checklu[i] = 1\n","        if all(checklu):\n","            textloc = (100, 60) #upper left\n","        else:\n","            textloc = (img.width-x-50, 30) #upper right\n","        \n","    pos = np.array(textloc)\n","    #insert the text\n","    bw = 1\n","    draw.text(pos-(-bw, -bw), message, font=font, fill='black')\n","    draw.text(pos-(-bw, +bw), message, font=font, fill='black')\n","    draw.text(pos-(+bw, -bw), message, font=font, fill='black')\n","    draw.text(pos-(+bw, +bw), message, font=font, fill='black')\n","    draw.text(pos, message, font=font, fill=(0, 0, 255, 0))\n","    img = np.array(img) # convert PIL into cv2(NumPy)\n","    cv2.imwrite(\"result/recommend_\" + emotion + \"/000\" + str(j) + \".jpg\", img)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"K4pHhxf438Me"},"source":["data_dir = \"processing/step3/search_data/\"\n","os.makedirs(data_dir, exist_ok=True)\n","\n","ssl._create_default_https_context = ssl._create_unverified_context\n","\n","def image(search_word, num):\n","    src_list = []\n","    Res = requests.get(\"https://www.google.com/search?hl=jp&q=\" + search_word + \"&btnG=Google+Search&tbs=0&safe=off&tbm=isch\")\n","    Html = Res.text\n","    Soup = bs4.BeautifulSoup(Html,'lxml')\n","    links = Soup.find_all(\"img\")            \n","    \n","    i = 0\n","    cnt = 0\n","    while cnt < num:\n","        src = links[i].get(\"src\")\n","        if src[len(src)-3:] == \"gif\":\n","            i += 1\n","            continue\n","        else:\n","            src_list.append(src)\n","            i += 1\n","            cnt += 1\n","    return src_list\n","\n","def download_img(url, file_name):\n","    r = requests.get(url, stream=True)\n","    if r.status_code == 200:\n","        with open(file_name +\".jpg\", 'wb') as f:\n","            r.raw.decode_content = True\n","            shutil.copyfileobj(r.raw, f)\n","            \n","num = 5\n","srcs = image(search_word, num)\n","for i in range(num):\n","    file_name = data_dir + str(i)\n","    download_img(srcs[i], file_name)\n","print(\"Image-Search Finished.\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3L7iWpRi38Mh"},"source":["# calculation of the similarity\n","def average_hash(target_file, size):\n","    img = Image.open(target_file)\n","    img = img.convert('RGB')\n","    img = img.resize((size, size), Image.ANTIALIAS)\n","    px = np.array(img.getdata()).reshape((size, size, 3))\n","    avg = px.mean()\n","    px = 1 * (px > avg)\n","    return px\n","\n","def hamming_dist(a, b):    \n","    a = a.reshape(1, -1)\n","    b = b.reshape(1, -1)\n","    dist = (a != b).sum()\n","    return dist\n","\n","size = 64\n","images = glob.glob(os.path.join(data_dir, \"*.jpg\"))\n","rate = 20.0\n","result = []\n","diffmin = 10000\n","\n","for j, targetf in enumerate(images):\n","    target_dist = average_hash(targetf, size)\n","    for i, fname in enumerate(images, j+3):\n","        dist = average_hash(fname, size)\n","        diff = hamming_dist(target_dist, dist)/256\n","        result.append([diff, targetf, fname])\n","        if diff < diffmin and diff != 0:\n","            diffmin = diff\n","            choice1 = targetf\n","            choice2 = fname\n","\n","print(\"Successful Image Selected.\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ol1MBEYJ38Mj"},"source":["# Inserting a specified image on an image in OpenCV format\n","class CvOverlayImage(object):\n","\n","    def __init__(self):\n","        pass\n","\n","    @classmethod\n","    def overlay(\n","            cls,\n","            cv_background_image,\n","            cv_overlay_image,\n","            point,\n","    ):\n","        \n","        overlay_height, overlay_width = cv_overlay_image.shape[:2]\n","\n","        # background image\n","        cv_rgb_bg_image = cv2.cvtColor(cv_background_image, cv2.COLOR_BGR2RGB)\n","        pil_rgb_bg_image = Image.fromarray(cv_rgb_bg_image)\n","        pil_rgba_bg_image = pil_rgb_bg_image.convert('RGBA')\n","        # foreground image\n","        cv_rgb_ol_image = cv2.cvtColor(cv_overlay_image, cv2.COLOR_BGRA2RGBA)\n","        pil_rgb_ol_image = Image.fromarray(cv_rgb_ol_image)\n","        pil_rgba_ol_image = pil_rgb_ol_image.convert('RGBA')\n","\n","        pil_rgba_bg_temp = Image.new('RGBA', pil_rgba_bg_image.size,\n","                                     (255, 255, 255, 0))\n","        pil_rgba_bg_temp.paste(pil_rgba_ol_image, point, pil_rgba_ol_image)\n","        result_image = \\\n","            Image.alpha_composite(pil_rgba_bg_image, pil_rgba_bg_temp)\n","\n","\n","        cv_bgr_result_image = cv2.cvtColor(\n","            np.asarray(result_image), cv2.COLOR_RGBA2BGRA)\n","\n","        return cv_bgr_result_image"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"t0k6NIhA38Mm"},"source":["response = requests.post(\n","    'https://api.remove.bg/v1.0/removebg',\n","    files={'image_file': open(choice2, 'rb')},\n","    data={'size': 'auto'},\n","    headers={'X-Api-Key': removebg_key},\n",")\n","if response.status_code == requests.codes.ok:\n","    with open(\"processing/step3/paste_img.png\", 'wb') as out:\n","        out.write(response.content)\n","    print(\"Successful Cutout\")\n","else:\n","    print(\"Error:\", response.status_code, response.text)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7viX0ftF38Mo"},"source":["cutimage = Image.open(\"processing/step3/paste_img.png\")\n","\n","crop = cutimage.split()[-1].getbbox()\n","newimage = cutimage.crop(crop)\n","newimage.save(\"processing/step3/paste_img.png\", quality=95)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ETxR4EFQ38Mq"},"source":["# paste the foreground image\n","def img_paste(background, location): \n","    foreground = cv2.imread(\"processing/step3/paste_img.png\", cv2.IMREAD_UNCHANGED)\n","    original_h, original_w = foreground.shape[:2]\n","    if original_h/original_w>1.5:\n","        foreground = cv2.resize(foreground, (int(background.shape[0]*0.7*foreground.shape[1]/foreground.shape[0]),int(background.shape[0]*0.7)))\n","    elif original_h/original_w>1:\n","        foreground = cv2.resize(foreground, (int(background.shape[0]*0.55*foreground.shape[1]/foreground.shape[0]),int(background.shape[0]*0.55)))\n","    else:\n","        foreground = cv2.resize(foreground, (int(background.shape[0]*0.4*foreground.shape[1]/foreground.shape[0]),int(background.shape[0]*0.4)))\n","    fore_h, fore_w = foreground.shape[:2]\n","    length = len(location)\n","    checkl = np.zeros(length)\n","\n","    for i in range(length):\n","        if fore_w + 100 < location[i][0]:\n","            checkl[i] = 1\n","    if all(checkl):\n","        point = (100, int((background.shape[0] - fore_h)/2)) #left\n","    else:\n","        point = (background.shape[1]-fore_w-100, int((background.shape[0] - fore_h)/2)) #right\n","    image = CvOverlayImage.overlay(background, foreground,point)\n","    return image"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jAvqm4MH-Vwu"},"source":["# Step 4: Text Insertion\n","\n","The thumbnails which are recommended to users are saved in the following folder:\n","\n","- **happiness**: the \"result/recommend_happiness\" folder\n","\n","- **surprise**: the \"result/recommend_surprise\" folder"]},{"cell_type":"code","metadata":{"id":"bJsawnt338Ma"},"source":["# summarize the title\n","def summary():\n","    headers = {\"x-api-key\": tsuna_key}\n","    url = \"https://clapi.asahi.com/headline-generation\"\n","    query = {\"text\" : summary_text, \"types\" : \"paper\", \"length\" : \"8\", \"n_head\": 5}\n","    r = requests.post(url, headers=headers, data=query)\n","    data = json.loads(r.text)\n","\n","    return data[\"headline\"]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wd3v5ZHe38Ms"},"source":["title_data = summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9ii8hNSE38Mu"},"source":["#original image\n","happiness = 'processing/step2/happiness.jpg'\n","surprise = 'processing/step2/surprise.jpg'\n","\n","face_cascade = cv2.CascadeClassifier('system/haarcascade_frontalface_default.xml')\n","\n","# happiness\n","happiness_img = cv2.imread(happiness, cv2.IMREAD_UNCHANGED)\n","location_happiness = detect_face(happiness_img)\n","os.makedirs(\"result/recommend_happiness\", exist_ok=True)\n","h_image = img_paste(happiness_img, location_happiness)\n","for i in range(0, 5):\n","    img_add_msg(h_image, title_data[i], location_happiness, \"happiness\", i)\n","    \n","#surprise\n","surprise_img = cv2.imread(surprise, cv2.IMREAD_UNCHANGED)\n","location_surprise = detect_face(surprise_img)\n","os.makedirs(\"result/recommend_surprise\", exist_ok=True)\n","s_image = img_paste(surprise_img, location_surprise)\n","for i in range(0, 5):\n","    img_add_msg(s_image, title_data[i], location_surprise, \"surprise\", i)\n","    \n","print(\"Output Finished.\")"],"execution_count":null,"outputs":[]}]}